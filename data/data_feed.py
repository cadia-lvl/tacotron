import os
import numpy as np
import argparse
import random
from text.text_tools import text_to_onehot
from hparams import hparams


class DataFeeder:
    '''
        Feeds batches from the dataset that has been
        generated at the in_dir path
    '''

    def __init__(self, in_dir):
        self._in_dir = in_dir
        self._metadata = load_metadata(os.path.join(in_dir, 'train.txt'))
        random.shuffle(self._metadata)
        self._cursor = 0 # index of the next sample
        self._num_samples = len(self._metadata)
        self._hparams = hparams
        self.batch_size = hparams.batch_size
        self.superbatch_size = hparams.superbatch_size

    def _get_next_superbatch(self):
        '''
            Get the next superbatch which is defined as a batch
            of batches. The size of superbatches is set in hparams.
        '''
        # TODO: Think about bucketing samples based on similar output
        # sequence length to improve running time
        return [self._get_next_batch() for _ in range(self.superbatch_size)]
        

    def _get_next_batch(self):
        '''
            Get the next batch. The batchsize is set in hparams.
        '''
        return [self._get_next_sample() for _ in range(self.batch_size)] 


    def _get_next_sample(self):
        '''
            Loads a single sample from the dataset
            
            Output:
            (Onehot text input, mel target, linear target, cost)
        '''
        lin_target_path, mel_target_path, n_frames, text = self._metadata[self._cursor] 
        self.increment_cursor()

        lin_target = np.load(os.path.join(self._in_dir, lin_target_path))
        mel_target = np.load(os.path.join(self._in_dir, mel_target_path))
        onehot_text = text_to_onehot(text)
        return (onehot_text, mel_target, lin_target, n_frames)
    
    def increment_cursor(self):
        '''
            Increments the dataset cursor, or sets it
            to 0 if we have reached the end of the dataset
        '''
        if self._cursor >= self._num_samples:
            # start from beginning and shuffle the
            # data again
            self._cursor = 0
            random.shuffle(self._metadata)
        else:   
            self._cursor += 1


def load_metadata(path):
    '''
        Loads the metadata generated by the prep functions
        at the given path
    '''
    with open(path, encoding='utf-8') as f:
      metadata = [line.strip().split('|') for line in f]
      #hours = sum((int(x[2]) for x in self._metadata)) * hparams.frame_shift_ms / (3600 * 1000)
      #log('Loaded metadata for %d examples (%.2f hours)' % (len(self._metadata), hours))
    return metadata