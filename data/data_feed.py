import argparse
import os
import random
import threading
import traceback
import time

import numpy as np
import tensorflow as tf

from data.batch import Batch
from hparams import hparams
from text.text_tools import text_to_onehot


class DataFeeder(threading.Thread):
    '''
        Feeds batches from the dataset that has been
        generated at the in_dir path
    '''

    def __init__(self, coordinator, in_dir, logger):
        super(DataFeeder, self).__init__()
        self._coordinator = coordinator
        self._in_dir = in_dir
        self._logger = logger
        self._metadata = load_metadata(os.path.join(in_dir, 'train.txt'), self._logger)
        random.shuffle(self._metadata)
        self._cursor = 0 # index of the next sample
        self._num_samples = len(self._metadata)
        self._hparams = hparams
        self.batch_size = hparams.get('batch_size')
        self.superbatch_size = hparams.get('superbatch_size')
        self.outputs_per_step = hparams.get('outputs_per_step')

        # Placeholders for inputs and targets.
        self._placeholders = [
            tf.placeholder(tf.int32, [None, None], 'inputs'),
            tf.placeholder(tf.int32, [None], 'input_lengths'),
            tf.placeholder(tf.float32, [None, None, hparams.get('num_mels')], 'mel_targets'),
            tf.placeholder(tf.float32, [None, None, hparams.get('num_freq')], 'linear_targets')
        ]

        # Create queue of capacity 8 for buffering data which
        # will buffer 8 superbatches onto the FIFO queue
        queue = tf.FIFOQueue(8, [tf.int32, tf.int32, tf.float32, tf.float32], name='input_queue')
        self._enqueue_operation = queue.enqueue(self._placeholders)
        self.current_batch = Batch(queue.dequeue(), prep=False)
        self.current_batch.set_shapes(self._placeholders)      

    def start_in_session(self, session):
        self._session = session
        self.start()

    def run(self):
        '''
            Override of the threading.Thread run method
        '''
        try:
            while not self._coordinator.should_stop():
                self._enqueue_next_superbatch()
        except Exception as e:
            traceback.print_exc()
            self._coordinator.request_stop(e)


    def _enqueue_next_superbatch(self):
        '''
            Get the next superbatch (a list of batches). 
            The size of superbatches is set in hparams.
        '''
        start = time.time()
        superbatch =  [self._get_next_sample() for _ in range(self.superbatch_size*self.batch_size)]
        # sort the samples in the superbatch on length w.r.t. time
        superbatch.sort(key=lambda x: x[-1])
        # now bucket the batches in that order to improve efficiency
        batches = [Batch(superbatch[i:i+self.batch_size]) for i in range(0, len(superbatch), self.batch_size)]
        random.shuffle(batches)
        self._logger.log('Generated %d batches of size %d in %.03f sec' % (len(batches), self.batch_size, time.time() - start))
        for batch in batches:
            feed_dict = dict(zip(self._placeholders, batch.get_all()))
            self._session.run(self._enqueue_operation, feed_dict=feed_dict)

    def _get_next_sample(self):
        '''
            Loads a single sample from the dataset
            
            Output:
            (Onehot text input, mel target, linear target, cost)
        '''
        lin_target_path, mel_target_path, n_frames, text = self._metadata[self._cursor] 
        self.increment_cursor()
        lin_target = np.load(os.path.join(self._in_dir, lin_target_path))
        mel_target = np.load(os.path.join(self._in_dir, mel_target_path))
        onehot_text = text_to_onehot(text)
        return (onehot_text, mel_target, lin_target, n_frames)
    
    def increment_cursor(self):
        '''
            Increments the dataset cursor, or sets it
            to 0 if we have reached the end of the dataset
        '''
        if self._cursor >= self._num_samples - 1:
            # start from beginning and shuffle the
            # data again
            self._cursor = 0
            random.shuffle(self._metadata)
        else:   
            self._cursor += 1

def load_metadata(path, logger):
    '''
        Loads the metadata generated by the prep functions
        at the given path
    '''
    with open(path, encoding='utf-8') as f:
        metadata = [line.strip().split('|') for line in f]
        hours = sum((int(x[2]) for x in metadata)) * hparams.get('frame_shift_ms') / (3600 * 1000)
        logger.log('Loaded metadata for %d examples (%.2f hours)' % (len(metadata), hours))
    return metadata
